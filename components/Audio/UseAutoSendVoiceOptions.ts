import { useState, useRef, useEffect, useCallback } from "react";

interface UseAutoSendVoiceOptions {
  /**
   * Tiempo de silencio en milisegundos antes de enviar automÃ¡ticamente
   * @default 3000 (3 segundos)
   */
  silenceThreshold?: number;

  /**
   * Umbral de detecciÃ³n de voz (0-255)
   * @default 8
   */
  speechThreshold?: number;

  /**
   * Callback que se ejecuta cuando se completa la grabaciÃ³n y transcripciÃ³n
   */
  onTranscriptionComplete?: (transcript: string) => void;

  /**
   * Callback que se ejecuta en caso de error
   */
  onError?: (error: Error) => void;

  /**
   * FunciÃ³n de transcripciÃ³n personalizada
   */
  transcriptionService: (audioBlob: Blob) => Promise<string>;

  /**
   * FunciÃ³n para detener la grabaciÃ³n del MediaRecorder
   */
  stopRecording: () => Promise<Blob>;

  /**
   * FunciÃ³n para iniciar la grabaciÃ³n del MediaRecorder
   */
  startRecording: (onChunk?: (chunk: Blob) => void) => Promise<MediaStream>;

  /**
   * Habilitar transcripciÃ³n en tiempo real (experimental)
   * @default false
   */
  enableRealtimeTranscription?: boolean;
}

interface UseAutoSendVoiceReturn {
  isRecording: boolean;
  isTranscribing: boolean;
  audioLevel: number;
  transcript: string;
  startVoiceRecording: () => Promise<void>;
  cancelVoiceRecording: () => Promise<void>;
  cleanup: () => void;
}

export function useAutoSendVoice({
  silenceThreshold = 3000,
  speechThreshold = 3, //
  onTranscriptionComplete,
  onError,
  transcriptionService,
  stopRecording,
  startRecording,
  enableRealtimeTranscription = false,
}: UseAutoSendVoiceOptions): UseAutoSendVoiceReturn {
  // ==================== ESTADOS ====================
  const [isRecording, setIsRecording] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  const [transcript, setTranscript] = useState("");

  // ==================== REFS ====================
  const isRecordingRef = useRef(false);
  const wasCancelledRef = useRef(false);
  const isProcessingRef = useRef(false);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const micStreamRef = useRef<MediaStream | null>(null);
  const recognitionRef = useRef<any>(null);
  const realtimeChunksRef = useRef<Blob[]>([]);
  const realtimeIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const mimeTypeRef = useRef<string>("");
  const transcriptRef = useRef("");

  // ==================== SINCRONIZACIÃ“N STATE <-> REF ====================
  useEffect(() => {
    isRecordingRef.current = isRecording;
  }, [isRecording]);

  useEffect(() => {
    transcriptRef.current = transcript;
  }, [transcript]);

  // ==================== RECONOCIMIENTO DE VOZ EN TIEMPO REAL ====================

  const updateTranscript = useCallback(
    (updater: string | ((prev: string) => string)) => {
      setTranscript((prev) => {
        const next = typeof updater === "function" ? updater(prev) : updater;
        transcriptRef.current = next;
        return next;
      });
    },
    [],
  );

  const startRealtimeWhisper = useCallback(() => {
    if (!enableRealtimeTranscription) return;

    realtimeChunksRef.current = [];

    realtimeIntervalRef.current = setInterval(async () => {
      if (!isRecordingRef.current || realtimeChunksRef.current.length === 0)
        return;

      const chunks = [...realtimeChunksRef.current];
      realtimeChunksRef.current = [];

      const chunkBlob = new Blob(chunks, {
        type: mimeTypeRef.current || "audio/webm",
      });

      // âœ… MÃ­nimo 10KB â€” chunks muy pequeÃ±os causan 400 en Groq
      if (chunkBlob.size < 10000) {
        // Devolver los chunks al buffer para acumular mÃ¡s
        realtimeChunksRef.current = [...chunks, ...realtimeChunksRef.current];
        return;
      }

      try {
        const partialTranscript = await transcriptionService(chunkBlob);
        if (partialTranscript?.trim()) {
          setTranscript((prev) => (prev + " " + partialTranscript).trim());
        }
      } catch {
        // silencioso
      }
    }, 3000); // âœ… Subir a 3 segundos para acumular mÃ¡s audio
  }, [enableRealtimeTranscription, transcriptionService]);

  const stopRealtimeWhisper = useCallback(() => {
    if (realtimeIntervalRef.current) {
      clearInterval(realtimeIntervalRef.current);
      realtimeIntervalRef.current = null;
    }
  }, []);

  const stopRealtimeRecognition = useCallback(() => {
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      } catch (error) {
        console.error("Error al detener reconocimiento:", error);
      }
    }
  }, []);

  const startRealtimeRecognition = useCallback(() => {
    if (!enableRealtimeTranscription) return;
    if (typeof window === "undefined") return;

    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;

    if (!SpeechRecognition) {
      startRealtimeWhisper();
      return;
    }

    // âœ… Timeout: si en 5 segundos no llega ningÃºn resultado, cambiar a Whisper
    let hasReceivedResult = false;
    const fallbackTimer = setTimeout(() => {
      if (!hasReceivedResult) {
        console.warn(
          "SpeechRecognition no produjo resultados, cambiando a Whisper",
        );
        stopRealtimeRecognition();
        startRealtimeWhisper();
      }
    }, 5000);

    try {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = "es-ES";

      recognition.onresult = (event: any) => {
        hasReceivedResult = true;
        clearTimeout(fallbackTimer); // âœ… Funciona, cancelar fallback

        let interimTranscript = "";
        let finalTranscript = "";
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcriptPart = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcriptPart + " ";
          } else {
            interimTranscript += transcriptPart;
          }
        }
        setTranscript((prev) => {
          const newTranscript = (prev + finalTranscript).trim();
          return (
            newTranscript + (interimTranscript ? " " + interimTranscript : "")
          );
        });
      };

      recognition.onerror = () => {
        clearTimeout(fallbackTimer);
        stopRealtimeRecognition();
        startRealtimeWhisper();
      };

      recognition.onend = () => {
        if (isRecordingRef.current && recognitionRef.current) {
          try {
            recognitionRef.current.start();
          } catch {}
        }
      };

      recognition.start();
      recognitionRef.current = recognition;
    } catch {
      clearTimeout(fallbackTimer);
      startRealtimeWhisper();
    }
  }, [
    enableRealtimeTranscription,
    startRealtimeWhisper,
    stopRealtimeRecognition,
  ]);

  // ==================== PROCESAMIENTO Y ENVÃO ====================
  const processAndSendAudio = useCallback(async () => {
    if (isProcessingRef.current) {
      return;
    }

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }

    isProcessingRef.current = true;
    setIsRecording(false);
    isRecordingRef.current = false;
    setIsTranscribing(true);
    setAudioLevel(0);

    // Detener reconocimiento en tiempo real
    stopRealtimeRecognition();

    // Detener detecciÃ³n de audio
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    if (audioContextRef.current) {
      await audioContextRef.current.close();
      audioContextRef.current = null;
    }
    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((track) => track.stop());
      micStreamRef.current = null;
    }

    try {
      // Obtener el audio grabado
      const audioBlob = await stopRecording();

      if (wasCancelledRef.current) {
        wasCancelledRef.current = false;
        setTranscript("");
        return;
      }

      // Si no hay transcripciÃ³n en tiempo real, usar el servicio de transcripciÃ³n
      let finalTranscript = transcriptRef.current.trim();

      if (!enableRealtimeTranscription || !finalTranscript) {
        finalTranscript = await transcriptionService(audioBlob);
      }

      if (finalTranscript && finalTranscript.length > 0) {
        setTranscript(finalTranscript);
        onTranscriptionComplete?.(finalTranscript);
      } else {
        onError?.(new Error("La transcripciÃ³n estÃ¡ vacÃ­a"));
      }
    } catch (error) {
      onError?.(
        error instanceof Error ? error : new Error("Error desconocido"),
      );
    } finally {
      setIsTranscribing(false);
      isProcessingRef.current = false;
      // Limpiar transcript despuÃ©s de enviar
      setTimeout(() => setTranscript(""), 500);
    }
  }, [
    silenceThreshold,
    stopRecording,
    transcriptionService,
    onTranscriptionComplete,
    onError,
    enableRealtimeTranscription,
    stopRealtimeRecognition,
  ]);

  // ==================== TIMER DE SILENCIO ====================
  const resetSilenceTimer = useCallback(() => {
    if (isProcessingRef.current) return;

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
    }

    silenceTimerRef.current = setTimeout(() => {
      processAndSendAudio();
    }, silenceThreshold);
  }, [silenceThreshold, processAndSendAudio]);

  // ==================== DETECCIÃ“N DE NIVEL DE AUDIO ====================
  const startAudioLevelDetection = useCallback(
    (stream: MediaStream) => {
      try {
        micStreamRef.current = stream;

        const audioContext = new AudioContext();
        const analyser = audioContext.createAnalyser();
        const microphone = audioContext.createMediaStreamSource(stream);

        analyser.fftSize = 512;
        analyser.smoothingTimeConstant = 0.8;
        microphone.connect(analyser);

        audioContextRef.current = audioContext;
        analyserRef.current = analyser;

        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        let frameCount = 0;

        // FunciÃ³n de detecciÃ³n continua
        const checkAudioLevel = () => {
          if (!isRecordingRef.current) {
            setAudioLevel(0);
            return;
          }

          analyser.getByteFrequencyData(dataArray);

          const sum = dataArray.reduce((a, b) => a + b, 0);
          const average = sum / dataArray.length;
          const normalizedLevel = Math.min(average * 2, 100);

          setAudioLevel(normalizedLevel);

          frameCount++;

          if (average > speechThreshold) {
            // ðŸŽ¤ VOZ DETECTADA
            resetSilenceTimer();
          }

          animationFrameRef.current = requestAnimationFrame(checkAudioLevel);
        };

        resetSilenceTimer();
        animationFrameRef.current = requestAnimationFrame(checkAudioLevel);
      } catch (error) {
        onError?.(
          error instanceof Error
            ? error
            : new Error("Error en detecciÃ³n de audio"),
        );
      }
    },
    [speechThreshold, resetSilenceTimer, onError],
  );

  // ==================== INICIAR GRABACIÃ“N ====================
  const startVoiceRecording = useCallback(async () => {
    wasCancelledRef.current = false;
    setTranscript(""); // Limpiar transcript anterior

    // Evitar inicio si ya se estÃ¡ procesando
    if (isTranscribing || isProcessingRef.current) {
      return;
    }

    // Si estÃ¡ grabando, cancelar y reiniciar
    if (isRecording) {
      await cancelVoiceRecording();
      await new Promise((resolve) => setTimeout(resolve, 100));
    }

    try {
      const stream = await startRecording((chunk) => {
        realtimeChunksRef.current.push(chunk); // acumular chunks para Whisper parcial
      });

      isRecordingRef.current = true;
      setIsRecording(true);

      startAudioLevelDetection(stream);

      if (
        (window as any).SpeechRecognition ||
        (window as any).webkitSpeechRecognition
      ) {
        startRealtimeRecognition(); // Chrome/Edge
      } else {
        startRealtimeWhisper(); // Opera/Firefox/otros
      }
    } catch (error) {
      let errorMessage = "No se pudo acceder al micrÃ³fono";

      if (error instanceof Error) {
        if (error.name === "NotAllowedError") {
          errorMessage =
            "Permisos denegados. Por favor, permite el acceso al micrÃ³fono.";
        } else if (error.name === "NotFoundError") {
          errorMessage =
            "No se encontrÃ³ ningÃºn micrÃ³fono. Conecta uno e intenta de nuevo.";
        } else {
          errorMessage = error.message;
        }
      }

      onError?.(new Error(errorMessage));

      setIsRecording(false);
      isRecordingRef.current = false;
      setAudioLevel(0);
      setTranscript("");
    }
  }, [
    isTranscribing,
    isRecording,
    startRecording,
    startAudioLevelDetection,
    startRealtimeRecognition,
    onError,
  ]);

  // ==================== CANCELAR GRABACIÃ“N ====================
  const cancelVoiceRecording = useCallback(async () => {
    wasCancelledRef.current = true;

    stopRealtimeRecognition();

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    if (audioContextRef.current) {
      await audioContextRef.current.close();
      audioContextRef.current = null;
    }
    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((track) => track.stop());
      micStreamRef.current = null;
    }

    await stopRecording();
    setAudioLevel(0);
    setIsRecording(false);
    isRecordingRef.current = false;
    setTranscript("");
  }, [stopRecording, stopRealtimeRecognition]);

  // ==================== LIMPIEZA ====================
  const cleanup = useCallback(() => {
    stopRealtimeRecognition();

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
    }

    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
    }

    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((track) => track.stop());
    }

    setTranscript("");
  }, [stopRealtimeRecognition]);

  // ==================== EFECTO DE MONTAJE/DESMONTAJE ====================
  useEffect(() => {
    return () => {
      cleanup();
    };
  }, [cleanup]);

  return {
    isRecording,
    isTranscribing,
    audioLevel,
    transcript,
    startVoiceRecording,
    cancelVoiceRecording,
    cleanup,
  };
}
