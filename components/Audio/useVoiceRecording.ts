// hooks/useVoiceRecording.ts
import { useState, useRef, useEffect } from "react";
import { useAudioRecorder } from "../hooks/useAudioRecorder";
import { transcribirAudioCliente } from "@/lib/transcription";

interface UseVoiceRecordingOptions {
  /**
   * Callback que se ejecuta cuando la transcripciÃ³n se completa exitosamente
   */
  onTranscriptionComplete: (text: string) => void;

  /**
   * Callback opcional que se ejecuta cuando hay un error
   */
  onError?: (error: Error) => void;

  /**
   * Tiempo de silencio en milisegundos antes de enviar el audio
   * @default 3000
   */
  silenceTimeout?: number;

  /**
   * Umbral de detecciÃ³n de voz (0-255). Valores mÃ¡s altos = menos sensible
   * @default 8
   */
  speechThreshold?: number;

  /**
   * Habilitar logs de debug en la consola
   * @default false
   */
  enableDebugLogs?: boolean;
}

interface UseVoiceRecordingReturn {
  /** Si estÃ¡ grabando actualmente */
  isRecording: boolean;

  /** Si estÃ¡ transcribiendo el audio */
  isTranscribing: boolean;

  /** Nivel de audio actual (0-100) */
  audioLevel: number;

  /** Si estÃ¡ procesando (grabando o transcribiendo) */
  isProcessing: boolean;

  /** Iniciar grabaciÃ³n de voz */
  startVoiceRecording: () => Promise<void>;

  /** Detener grabaciÃ³n de voz */
  stopVoiceRecording: () => Promise<void>;

  /** Alternar grabaciÃ³n (si estÃ¡ grabando -> reinicia, si no -> inicia) */
  toggleVoiceRecording: () => Promise<void>;
}

export const useVoiceRecording = ({
  onTranscriptionComplete,
  onError,
  silenceTimeout = 3000,
  speechThreshold = 8,
  enableDebugLogs = false,
}: UseVoiceRecordingOptions): UseVoiceRecordingReturn => {
  // ==========================================
  // DEPENDENCIAS
  // ==========================================
  const { startRecording, stopRecording } = useAudioRecorder();

  // ==========================================
  // ESTADOS
  // ==========================================
  const [isRecording, setIsRecording] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);

  // ==========================================
  // REFS
  // ==========================================
  const isRecordingRef = useRef(false);
  const isProcessingRef = useRef(false);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const micStreamRef = useRef<MediaStream | null>(null);

  // ==========================================
  // HELPER: LOGGING CONDICIONAL
  // ==========================================
  const log = (...args: any[]) => {
    if (enableDebugLogs) {
      console.log(...args);
    }
  };

  // ==========================================
  // SINCRONIZAR REF CON STATE
  // ==========================================
  useEffect(() => {
    isRecordingRef.current = isRecording;
    log("ðŸ”„ [SYNC] isRecordingRef actualizado:", isRecording);
  }, [isRecording]);

  // ==========================================
  // LIMPIAR RECURSOS
  // ==========================================
  const cleanupResources = () => {
    log("ðŸ§¹ [CLEANUP] Limpiando recursos");

    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }

    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((track) => track.stop());
      micStreamRef.current = null;
    }
  };

  // ==========================================
  // PROCESAR Y ENVIAR AUDIO
  // ==========================================
  const processAndSendAudio = async () => {
    if (isProcessingRef.current) {
      log("âš ï¸ Ya se estÃ¡ procesando audio");
      return;
    }

    log("ðŸ“¤ [PROCESO] Enviando audio despuÃ©s de silencio");
    isProcessingRef.current = true;
    setIsRecording(false);
    isRecordingRef.current = false;
    setIsTranscribing(true);
    setAudioLevel(0);

    cleanupResources();

    try {
      const audio = await stopRecording();
      log("ðŸ”„ [TRANSCRIPCIÃ“N] Transcribiendo audio...");

      const result = await transcribirAudioCliente(audio);

      if (result && result.trim().length > 0) {
        log("âœ… [TRANSCRIPCIÃ“N] Texto:", result);
        onTranscriptionComplete(result);
      } else {
        log("âš ï¸ [TRANSCRIPCIÃ“N] Texto vacÃ­o o nulo");
      }
    } catch (error) {
      console.error("âŒ [ERROR] Error al transcribir:", error);
      onError?.(error as Error);
    } finally {
      setIsTranscribing(false);
      isProcessingRef.current = false;
      log("âœ… [PROCESO] Proceso completado");
    }
  };

  // ==========================================
  // REINICIAR TIMER DE SILENCIO
  // ==========================================
  const resetSilenceTimer = () => {
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
    }

    log(`â±ï¸ [TIMER] Reiniciado - esperando ${silenceTimeout}ms de silencio`);

    silenceTimerRef.current = setTimeout(() => {
      log("âœ… [TIMER] Tiempo completado - enviando audio");
      processAndSendAudio();
    }, silenceTimeout);
  };

  // ==========================================
  // DETECTAR NIVEL DE AUDIO
  // ==========================================
  const startAudioLevelDetection = (stream: MediaStream) => {
    try {
      log("ðŸŽ¤ [DETECCIÃ“N] Configurando detecciÃ³n de audio");

      log("ðŸ” [DIAG] Stream recibido:", {
        id: stream.id,
        active: stream.active,
        tracks: stream.getTracks().map((t) => ({
          kind: t.kind,
          label: t.label,
          enabled: t.enabled,
          muted: t.muted,
          readyState: t.readyState,
        })),
      });

      micStreamRef.current = stream;

      const audioContext = new AudioContext();
      log("ðŸ” [DIAG] AudioContext state:", audioContext.state);

      const analyser = audioContext.createAnalyser();
      const microphone = audioContext.createMediaStreamSource(stream);

      analyser.fftSize = 512;
      analyser.smoothingTimeConstant = 0.3;
      microphone.connect(analyser);

      audioContextRef.current = audioContext;
      analyserRef.current = analyser;

      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      let frameCount = 0;

      // Test inmediato (solo si debug estÃ¡ habilitado)
      if (enableDebugLogs) {
        const testAudioImmediate = () => {
          analyser.getByteFrequencyData(dataArray);
          const sum = dataArray.reduce((a, b) => a + b, 0);
          const average = sum / dataArray.length;

          log("ðŸ” [DIAG] TEST INMEDIATO - Nivel promedio:", average.toFixed(2));
          log(
            "ðŸ” [DIAG] Primeros 10 valores del array:",
            dataArray.slice(0, 10),
          );

          if (average === 0) {
            console.error("âŒ [DIAG] PROBLEMA DETECTADO: Nivel es 0");
          } else {
            log("âœ… [DIAG] Audio funcionando correctamente");
          }
        };

        setTimeout(testAudioImmediate, 100);
        setTimeout(testAudioImmediate, 500);
      }

      // FunciÃ³n de detecciÃ³n de audio
      const checkAudioLevel = () => {
        if (!isRecordingRef.current) {
          log("âš ï¸ [DETECCIÃ“N] isRecording es false, deteniendo...");
          setAudioLevel(0);
          return;
        }

        analyser.getByteFrequencyData(dataArray);

        const sum = dataArray.reduce((a, b) => a + b, 0);
        const average = sum / dataArray.length;
        const normalizedLevel = Math.min(average * 2, 100);

        setAudioLevel(normalizedLevel);

        // Log primeros 10 frames (solo si debug estÃ¡ habilitado)
        if (enableDebugLogs && frameCount < 10) {
          log(
            `ðŸ” [DIAG] Frame ${frameCount} - Average: ${average.toFixed(2)}, Normalized: ${normalizedLevel.toFixed(2)}, isRecording: ${isRecordingRef.current}`,
          );
        }

        frameCount++;

        if (average > speechThreshold) {
          // Voz detectada
          log(
            `ðŸŽ¤ [VOZ] Nivel: ${average.toFixed(1)} | Normalizado: ${normalizedLevel.toFixed(1)} | Frame: ${frameCount}`,
          );
          resetSilenceTimer();
        } else {
          // Silencio
          if (enableDebugLogs && frameCount % 30 === 0) {
            log(
              `ðŸ”‡ [SILENCIO] Nivel: ${average.toFixed(1)} | Frame: ${frameCount}`,
            );
          }
        }

        animationFrameRef.current = requestAnimationFrame(checkAudioLevel);
      };

      resetSilenceTimer();
      checkAudioLevel();

      log("âœ… [DETECCIÃ“N] ConfiguraciÃ³n completada");
    } catch (error) {
      console.error("âŒ [ERROR] No se pudo configurar detecciÃ³n:", error);
      resetSilenceTimer();
    }
  };

  // ==========================================
  // INICIAR GRABACIÃ“N
  // ==========================================
  const startVoiceRecording = async () => {
    if (isTranscribing || isProcessingRef.current) {
      log("âš ï¸ Ignorando inicio - ya se estÃ¡ procesando");
      return;
    }

    log("ðŸŽ™ï¸ [INICIO] Iniciando nueva grabaciÃ³n...");

    try {
      log("ðŸ“ž Llamando a startRecording()...");
      const stream = await startRecording();

      log("âœ… Stream obtenido:", {
        id: stream.id,
        active: stream.active,
        tracks: stream.getTracks().map((t) => ({
          kind: t.kind,
          label: t.label,
          enabled: t.enabled,
        })),
      });

      setIsRecording(true);
      isRecordingRef.current = true;
      log("âœ… Estado isRecording = true, ref = true");

      log("ðŸŽ§ Iniciando detecciÃ³n de audio con el mismo stream...");
      startAudioLevelDetection(stream);

      log("âœ… [INICIO] GrabaciÃ³n iniciada correctamente");
    } catch (error) {
      console.error("âŒ [ERROR] Error al iniciar grabaciÃ³n:", error);

      setIsRecording(false);
      isRecordingRef.current = false;
      setAudioLevel(0);

      // Propagar error
      onError?.(error as Error);
      throw error;
    }
  };

  // ==========================================
  // DETENER GRABACIÃ“N
  // ==========================================
  const stopVoiceRecording = async () => {
    if (!isRecording) {
      log("âš ï¸ No hay grabaciÃ³n activa para detener");
      return;
    }

    log("ðŸ›‘ [STOP] Deteniendo grabaciÃ³n...");

    cleanupResources();

    await stopRecording();
    setIsRecording(false);
    isRecordingRef.current = false;
    setAudioLevel(0);

    log("âœ… [STOP] GrabaciÃ³n detenida");
  };

  // ==========================================
  // TOGGLE GRABACIÃ“N
  // ==========================================
  const toggleVoiceRecording = async () => {
    if (isTranscribing || isProcessingRef.current) {
      log("âš ï¸ Ignorando toggle - procesando...");
      return;
    }

    if (isRecording) {
      log("ðŸ”„ [TOGGLE] Cancelando y reiniciando grabaciÃ³n...");
      await stopVoiceRecording();
      await new Promise((resolve) => setTimeout(resolve, 100));
      await startVoiceRecording();
    } else {
      await startVoiceRecording();
    }
  };

  // ==========================================
  // LIMPIAR AL DESMONTAR
  // ==========================================
  useEffect(() => {
    log("âœ… [MOUNT] Hook de voz montado");

    return () => {
      log("ðŸ§¹ [UNMOUNT] Limpiando hook de voz");
      cleanupResources();
    };
  }, []);

  // ==========================================
  // RETORNAR API PÃšBLICA
  // ==========================================
  return {
    // Estados
    isRecording,
    isTranscribing,
    audioLevel,
    isProcessing: isProcessingRef.current || isTranscribing,

    // Acciones
    startVoiceRecording,
    stopVoiceRecording,
    toggleVoiceRecording,
  };
};
